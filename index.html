<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>ACMMM'22| Speech Fusion to Face</title>
<link href="css/style.css" rel="stylesheet" type="text/css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>
</head>

<body>
<div class="container">
<h3 align="center" style="font-family:Times-New-Roman;">
  Proceedings of the ACM Multimedia 2022
</h3>

<h1 align="center" style="font-family:Times-New-Roman;">
  Speech Fusion to Face: Bridging the Gap Between Human’s Vocal Characteristics and Facial Imaging
</h1>

<h2 align="center" style="font-family:Times-New-Roman;">
  Supplementary Material
</h2>

<p align="center">&nbsp;</p>

<p style="font-family:Times-New-Roman;">
In the main paper, we present a state-of-the-art algorithm for automatic
generation of facial images based on the vocal characteristics extracted from
human speech.
In this supplementary, we show the input audio that cannot be included
in the main paper, additional experiment results, and model details.
The input audio can be played in the browser (tested on Chrome and HTML5).
  
</p>
  
<p>
  <ul>
    <li>Open-source code: https://github.com/BAI-Yeqi/SF2F_PyTorch</li>
    <li>Open-source dataset: https://github.com/BAI-Yeqi/HQ-VoxCeleb</li>
  </ul>
</p>

<p>&nbsp;</p>

<h2>Contents</h2>
<p>
  <ul>
    <li><a href="#teaser">Teaser</a></li>
    <li><a href="#results">Qualitative Results</a></li>
    <li><a href="#data-quality">Data Quality Enhancement</a></li>
    <li><a href="#ablation">Additional Experimental Results</a></li>
    <li><a href="#model">Model Details</a></li>
    <li><a href="#imple">Implementation Details</a></li>
    <li><a href="#reference">References</a></li>
  </ul>
</p>
<p>&nbsp;</p>

<!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=--->
<h2><a id="teaser"></a>
  Teaser
</h2>
<table border="0" align="center">
  <tbody>
    <tr align="center">
      <td>&nbsp;</td>
      <td><strong>True face<br />
          (for reference)</strong></td>
      <td><strong>Face reconstructed<br />
        from speech</strong></td>
      <td width="24">&nbsp;</td>
      <td><strong>True face<br />
        (for reference)</strong></td>
      <td><strong>Face reconstructed<br />
        from speech</strong></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Bianca_Guaccero/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Bianca_Guaccero/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/CariDee_English/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/CariDee_English/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Bianca_Guaccero/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/CariDee_English/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Dominic_West/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Dominic_West/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Emiliano_Viviano/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Emiliano_Viviano/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Dominic_West/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Emiliano_Viviano/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Eric_Bana/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Eric_Bana/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Carolina_Crescentini/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Carolina_Crescentini/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Eric_Bana/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Carolina_Crescentini/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
  </tbody>
</table>
<p>
  <b>128 × 128 results on the VoxCeleb dataset.</b>
  Several results of SF2F are shown.
  SF2F can be genearlized from 64 × 64 pixel reconstruction to higher resolutions.
  Our method works for various lengths of input audio.
</p>
<p><a href="#">To the top.</a></p>
<p>&nbsp;</p>
<!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=--->

<h2><a id="results"></a>
  1 Qualitative Results
</h2>
<p>
  In the main paper, we compare the 64 × 64 face images generated by SF2F and
  voice2face [<a href="#reference" style="color: green; text-decoration: none;">1</a>].
  In this section, we compare the 128 × 128 face images reconstructed by both models.
  As the original voice2face [<a href="#reference" style="color: green; text-decoration: none;">1</a>, <a href="#reference" style="color: green; text-decoration: none;">2</a>]
  is only trained to generate 64 × 64 images, we compare SF2F and voice2face trained
  on HQ-VoxCeleb. For the sake of fairness, both models are trained until convergence, and checkpoints
  with the best L1 similarity is used for inference. As shown in Figure <a href="#figure-1" style="color: red; text-decoration: none;">1</a>,
  although voice2face can capture attributes such as gender, SF2F generates images
  with much more accurate facial features and face shape. The pose, expression,
  and lighting over the faces from SF2F are generally more stable and consistent than
  the face images from voice2face. In group (f), for example, SF2F predicts
  a face almost identical to the ground truth, when the corresponding output from
  voice2face is hardly recognizable. This proves our model design enables more accurate
  information flow from the speech domain to the face domain.
</p>
<table id="figure-1" border="0" align="center">
  <tbody>
    <tr align="center">
      <td width="50">&nbsp;</td>
      <td><strong>Ground Truth<br />
          (for reference)</strong></td>
      <td><strong>SF2F<br /></strong></td>
      <td><strong>voice2face<br /></strong></td>
      <td width="50">&nbsp;</td>
      <td><strong>Ground Truth<br />
        (for reference)</strong></td>
      <td><strong>SF2F<br /></strong></td>
      <td><strong>voice2face<br /></strong></td>
    </tr>
    <tr>
      <td style="text-align: center;">(a)</td>
      <td><img src="examples/mid/Aarti_Chhabria/gt.jpg" width="145" /></td>
      <td><img src="examples/mid/Aarti_Chhabria/gen.png" width="145" /></td>
      <td><img src="examples/mid/Aarti_Chhabria/v2f.png" width="145" /></td>
      <td style="text-align: right;">(e)</td>
      <td><img src="examples/mid/Adam_Senn/gt.jpg" width="145" /></td>
      <td><img src="examples/mid/Adam_Senn/gen.png" width="145" /></td>
      <td><img src="examples/mid/Adam_Senn/v2f.png" width="145" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="3"><audio controls="controls" style="width: 400px; margin-left: 20px;">
          <source src="examples/mid/Aarti_Chhabria/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="3"><audio controls="controls" style="width: 400px; margin-left: 20px;">
          <source src="examples/mid/Adam_Senn/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td style="text-align: center;">(b)</td>
      <td><img src="examples/mid/Alaina_Huffman/gt.jpg" width="145" /></td>
      <td><img src="examples/mid/Alaina_Huffman/gen.png" width="145" /></td>
      <td><img src="examples/mid/Alaina_Huffman/v2f.png" width="145" /></td>
      <td  style="text-align: right;">(f)</td>
      <td><img src="examples/mid/Annet_Mahendru/gt.jpg" width="145" /></td>
      <td><img src="examples/mid/Annet_Mahendru/gen.png" width="145" /></td>
      <td><img src="examples/mid/Annet_Mahendru/v2f.png" width="145" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="3"><audio controls="controls" style="width: 400px; margin-left: 20px;">
          <source src="examples/mid/Alaina_Huffman/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="3"><audio controls="controls" style="width: 400px; margin-left: 20px;">
          <source src="examples/mid/Annet_Mahendru/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td style="text-align: center;">(c)</td>
      <td><img src="examples/mid/Dean_Morgan/gt.jpg" width="145" /></td>
      <td><img src="examples/mid/Dean_Morgan/gen.png" width="145" /></td>
      <td><img src="examples/mid/Dean_Morgan/v2f.png" width="145" /></td>
      <td style="text-align: right;">(g)</td>
      <td><img src="examples/mid/Diego_Botto/gt.jpg" width="145" /></td>
      <td><img src="examples/mid/Diego_Botto/gen.png" width="145" /></td>
      <td><img src="examples/mid/Diego_Botto/v2f.png" width="145" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="3"><audio controls="controls" style="width: 400px; margin-left: 20px;">
          <source src="examples/mid/Dean_Morgan/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="3"><audio controls="controls" style="width: 400px; margin-left: 20px;">
          <source src="examples/mid/Diego_Botto/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td style="text-align: center;">(d)</td>
      <td><img src="examples/mid/Apolo_Ohno/gt.jpg" width="145" /></td>
      <td><img src="examples/mid/Apolo_Ohno/gen.png" width="145" /></td>
      <td><img src="examples/mid/Apolo_Ohno/v2f.png" width="145" /></td>
      <td style="text-align: right;">(h)</td>
      <td><img src="examples/mid/Ashley_Tisdale/gt.jpg" width="145" /></td>
      <td><img src="examples/mid/Ashley_Tisdale/gen.png" width="145" /></td>
      <td><img src="examples/mid/Ashley_Tisdale/v2f.png" width="145" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="3"><audio controls="controls" style="width: 400px; margin-left: 20px;">
          <source src="examples/mid/Apolo_Ohno/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="3"><audio controls="controls" style="width: 400px; margin-left: 20px;">
          <source src="examples/mid/Ashley_Tisdale/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
  </tbody>
</table>
<p style="text-align: center;">
  Figure 1: Examples of 128 × 128 generated images using SF2F and voice2face.
</p>
<p><a href="#">To the top.</a></p>
<p>&nbsp;</p>

<h2><a id="data-quality"></a>
  2 Data Quality Enhancement
</h2>
<h3> 2.1 HQ-VoxCeleb Dataset </h3>
<p>
  As an overview of the HQ-VoxCeleb dataset is presented in the main paper,
  in this section, we elaborate on the standards and process in our data enhancement scheme.
  As demonstrated in Figure <a href="#figure-2" style="color: red; text-decoration: none;">2</a>,
  the poor quality of training dataset for <i>speech2face</i> is one of the major factors
  hindering the improvement of <i>speech2face</i> performance. To eliminate the
  negative impact of the training dataset, we carefully design and build a new
  high-quality face database on top of VoxCeleb dataset, such that face images
  associated with the celebrities are all at reasonable quality.
</p>
<table id="figure-2" border="0" align="center">
  <tbody>
    <tr align="center">
      <td>&nbsp;</td>
      <td colspan="2">
        <strong><font size="4">HQ-VoxCeleb<font><br /></strong>
      </td>
      <td width="40">&nbsp;</td>
      <td colspan="2">
        <strong><font size="4">Filtered VGGFace<font><br /></strong>
      </td>
      <td width="40">&nbsp;</td>
      <td colspan="2">
        <strong><font size="4">Problems with Filtered VGGFace<font>
        <br /></strong>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Alex_Borstein/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Alex_Borstein/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Alex_Borstein/lq/3.jpg" width="125" /></td>
      <td><img src="examples/data/Alex_Borstein/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Expression</li>
          <li>Background</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Andrea_Riseborough/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Andrea_Riseborough/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Andrea_Riseborough/lq/1.jpg" width="125" /></td>
      <td><img src="examples/data/Andrea_Riseborough/lq/5.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Background</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Andrew_Rannells/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Andrew_Rannells/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Andrew_Rannells/lq/1.jpg" width="125" /></td>
      <td><img src="examples/data/Andrew_Rannells/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Expression</li>
          <li>Background</li>
          <li>Resolution</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Anthony_Anderson/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Anthony_Anderson/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Anthony_Anderson/lq/5.jpg" width="125" /></td>
      <td><img src="examples/data/Anthony_Anderson/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Expression</li>
          <li>Background</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Bear_Grylls/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Bear_Grylls/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Bear_Grylls/lq/1.jpg" width="125" /></td>
      <td><img src="examples/data/Bear_Grylls/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Expression</li>
          <li>Background</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Emraan_Hashmi/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Emraan_Hashmi/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Emraan_Hashmi/lq/3.jpg" width="125" /></td>
      <td><img src="examples/data/Emraan_Hashmi/lq/0.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Background</li>
          <li>Lighting condition</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Fawad_Khan/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Fawad_Khan/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Fawad_Khan/lq/4.jpg" width="125" /></td>
      <td><img src="examples/data/Fawad_Khan/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Background</li>
          <li>Resolution</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Freida_Pinto/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Freida_Pinto/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Freida_Pinto/lq/1.jpg" width="125" /></td>
      <td><img src="examples/data/Freida_Pinto/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Background</li>
          <li>Lighting condition</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
<p>
  Figure 2: The quality variance of the original manually filtered VGGFace dataset
  (on the right of the figure) [<a href="#reference" style="color: green; text-decoration: none;">3</a>] diverts the efforts of face generator model to
  the normalization of the faces. By filtering and correcting the face database
  (on the left), the computer vision models are expected to focus more on the construction
  of mapping between vocal features and physical facial features.
</p>
<p>
  To fulfill the vision of data quality, we set a number of general guidelines
  over the face images as the underlying measurement on <i>quality</i>, as follows:
  <ul>
    <li><b>Face angle</b> between human’s facial plane and the photo imaging plane is no larger than 5&deg;;</li>
    <li><b>Lighting condition</b> on the face is generally uniform, without any obvious shadow and sharp
illumination change;</li>
    <li><b>Expression</b> on the human faces is generally neutral, while minor smile expression is also acceptable;</li>
    <li><b>Background</b> does not contain irrelevant information in the image, and completely void background in white is preferred.</li>
  </ul>
</p>
<p>
  To fully meet the standards as listed above, we adopt the following methodology to build the enhanced
  <i>HQ-VoxCeleb</i> dataset for our <i>speech2face</i> model training.
</p>
<p>
  <b>Data Collection.</b> We collect 300 in-the-wild images for each of the 7,363
  individuals in VoxCeleb dataset, by crawling images of the celebrities on the
  Internet. The visual qualities of the retrieved images are highly diverse.
  The resolution of the images, for example, ranges from 47 × 59 to 6245 × 8093 in pixels.
  Moreover, some of the images cover the full body of the celebrity of interest,
  while other images only include the face of the target individual.
  It is, therefore, necessary to apply pre-processing and filtering operations
  to ensure 1) the variance of the image quality is reasonably small; and 2) all
  the images are centered at the face of the target individuals.
</p>
<p>
  <b>Machine Filtering.</b> To filter out unqualified images from the massive in-the-wild
  images, we deploy an automated filtering module, together with a suite of concrete
  selection rules, to eliminate images at poor quality, before dispatching the images
  for human filtering. In the filtering module, the algorithm first detects the
  landmarks from the raw face images. Based on the output landmarks of the faces,
  the algorithm identifies poorly posed faces, if the landmarks from left/right
  sides cannot be well aligned. Low-resolution face images with distance between
  pupils covers fewer than 30 pixels are also removed. Finally, a pre-trained CNN
  classifier [<a href="#reference" style="color: green; text-decoration: none;">4</a>] is
  deployed to infer the emotion of the individual in the image, such that faces
  not recognized as "neutral” emotion are also removed from the dataset.
</p>
<p>
  <b>Image Processing.</b> Given the face images passing the first round machine filtering,
  we apply a two-step image processing, namely face alignment and image segmentation.
  In the first step of face alignment, the images are rotated and cropped to make
  sure both the pupils of the faces in all these images are always at the same coordinates.
  In the second step of image segmentation, we apply a pyramid scene parsing
  network [<a href="#reference" style="color: green; text-decoration: none;">5</a>] pre-trained
  on Pascal VOC 2012 dataset [<a href="#reference" style="color: green; text-decoration: none;">6</a>]
  to split the target individual from the background in the image. The background
  is then refilled with white pixels. Note that the second step is helpful because
  irrelevant noise in the background may potentially confuse the generation model.
</p>
<p>
  <b>Human Filtering.</b> To guarantee the final quality of the images in <i>HQ-VoxCeleb</i> dataset, human
  workers are employed to select 1 to 7 images at best qualities for each celebrity identity.
  Only celebrities with at least 1 qualified face image are kept in the final dataset.
</p>
<h3>2.2 Comparison with Existing Datasets</h3>
<p>
  In the main paper, we summarize the statistics of the result dataset after the adoption of the processing
  steps above. In this section, we compare HQ-VoxCeleb with existing audiovisual
  datasets [<a href="#reference" style="color: green; text-decoration: none;">7</a>, <a href="#reference" style="color: green; text-decoration: none;">8</a>,
  <a href="#reference" style="color: green; text-decoration: none;">9</a>, <a href="#reference" style="color: green; text-decoration: none;">10</a>, <a href="#reference" style="color: green; text-decoration: none;">11</a>]
  in terms of data quality and its impact to model training, in order to justify the contribution of
  HQ-VoxCeleb. Table <a href="#table-1" style="color: red; text-decoration: none;">1</a> shows the attributes of existing audiovisual datasets.
</p>
<p>
  Existing datasets, including VoxCeleb [<a href="#reference" style="color: green; text-decoration: none;">11</a>, <a href="#reference" style="color: green; text-decoration: none;">10</a>]
  and AVSpeech [<a href="#reference" style="color: green; text-decoration: none;">9</a>], contain a massive number of
  pairwise data of human speech and face images. However, the existing datasets are constructed by
  cropping face images and speech audio from in-the-wild online data, and the face images thus vary
  hugely in pose, lighting, and emotion, which makes the existing datasets unfit for end-to-end learning
  of speech-to-face algorithms. To the highest image quality of existing datasets, Wen et al. [<a href="#reference" style="color: green; text-decoration: none;">1</a>] use
  the intersection of the filtered VGGFace [<a href="#reference" style="color: green; text-decoration: none;">7</a>] and VoxCeleb with the common identities. However,
  as shown in Fig. 2, the filtered VGGFace cannot meet the quality standards defined in section 2.1.
  Moreover, HQ-VoxCeleb has triple as many identities as filtered VGGFace as shown in Table <a href="#table-1" style="color: red; text-decoration: none;">1</a>.
  In conclusion, the high quality and reasonable amount of data make HQ-VoxCeleb the most suitable
  dataset for <i>speech2face</i> tasks.
</p>
<p>
  <div id="table-1" style="width: 750px; height: 230px; margin: 0 auto;">
    <img src="./figures/table-1.png" width="750"/>
  </div>
</p>
<p><a href="#">To the top.</a></p>
<p>&nbsp;</p>

<!-- Ablation Study -->
<h2><a id="ablation"></a>
  3 Additional Experimental Results
</h2>
<h3>3.1 Ablation Study</h3>
<p>
  In this section, we focus on the effectiveness evaluation over the model components, loss functions,
  and dataset used in our SF2F model. The ablated models are trained to generate
  64 × 64 images, with all the results are summarized in
  Table <a href="#table-2" style="color: red; text-decoration: none;">2</a>.
  When removing any of the four loss functions in {<i>L<sub>1</sub></i>, <i>L<sub>G</sub></i>,
  <i>L<sub>C</sub></i> , <i>L<sub>P</sub></i>}, the performance of SF2F drops accordingly.
  This shows it is necessary to include all these components to ensure the learning
  procedure of the model is well balanced. 1D-CNN encoder is used in the baseline
  approach [<a href="#reference" style="color: green; text-decoration: none;">1</a>],
  while SF2F employs 1D-Inception encoder instead. We report the performance of
  SF2F by replacing our encoder with 1D-CNN, referred to as <i>Baseline Encoder</i> in
  Table <a href="#table-2" style="color: red; text-decoration: none;">2</a>. This
  replacement causes a significant drop of the performance on all metrics. Similarly, by adopting
  the deconvolution-based decoder used in [<a href="#reference" style="color: green; text-decoration: none;">1</a>, <a href="#" style="color: green; text-decoration: none;">12</a>]
  instead of the upsampling-and-convolution-based decoder in SF2F, referred as
  Baseline Decoder in Table <a href="#table-2" style="color: red; text-decoration: none;">2</a>,
  we observe a slight yet consistent performance drop.
  The impact of data quality is also evaluated, by training the SF2F model with
  the manually filtered version of VGGFace dataset [<a href="#reference" style="color: green; text-decoration: none;">3</a>]
  instead of HQ-VoxCeleb, by including overlap celebrity individuals included in both datasets.
  Poor data quality obviously leads to a huge performance plunge, which further justifies the importance of training data quality enhancement.
</p>
<p>
  <div id="table-2" style="width: 600px; height: 300px; margin: 0 auto;">
    <img src="./figures/table-2.png" width="600"/>
  </div>
</p>
<h3>3.2 Performance on Filtered VGGFace</h3>
<p>
  We also compare the performance of SF2F and voice2face [<a href="#reference" style="color: green; text-decoration: none;">1</a>]
  on the filtered VGGFace dataset [<a href="#reference" style="color: green; text-decoration: none;">3</a>], to
  evaluate how SF2F functions under less controlled conditions. To make a fair comparison, we train
  both models with filtered VGGFace dataset. Both voice2face and our SF2F are trained to generate
  images with 64 × 64 pixels, and evaluated with 1.25 seconds and 5 seconds of human speech clips. As
  is presented in Table <a href="#table-3" style="color: red; text-decoration: none;">3</a>,
  SF2F outperforms voice2face by a large margin on all metrics. By deploying
  fuser in SF2F, the maximal recall@10 reaches 21.34%, significantly outperforming voice2face.
</p>
<p>
  <div id="table-3" style="width: 800px; height: 270px; margin: 0 auto;">
    <img src="./figures/table-3.png" width="800"/>
  </div>
</p>
<p><a href="#">To the top.</a></p>
<p>&nbsp;</p>

<!-- Model Details -->
<h2><a id="model"></a>
  4 Model Details
</h2>
<h3>4.1 Voice Encoder</h3>
<p>
  We use a 1D-CNN composed of several 1D Inception modules
  [<a href="#reference" style="color: green; text-decoration: none;">13</a>] to process mel-spectrogram.
  The voice encoder module converts each short input speech segment into a predicted
  facial feature embedding. The architecture of the 1D Inception module and the
  voice encoder is summarized in Table <a href="#table-4" style="color: red; text-decoration: none;">4</a>.
  The Inception module models various ranges
  of short-term mel-spectrogram dependency, and enables more accurate information
  flow from voice domain to image domain, compared to plain single-kernel-size CNN
  used in [<a href="#reference" style="color: green; text-decoration: none;">1</a>].
</p>
<p>
  <div id="table-4" style="width: 800px; height: 280px; margin: 0 auto;">
    <img src="./figures/table-4.png" width="800"/>
  </div>
</p>

<h3>4.2 Face Decoder</h3>
<h3>4.2.1 Single-resolution Decoder</h3>
<p>
  The face decoder reconstructs the target individual’s face image based on the
  embeddings extracted from the individual’s speech segments. The architectures
  of the upsampling block (UpBlock) and the face decoder are summarized in
  Table <a href="#table-5" style="color: red; text-decoration: none;">5</a>.
  We show the structure of the face decoder that generates 64 × 64 images, and the
  face decoder generates 128 × 128 images can be built by adding an <i>UpBlock 7</i> after
  <i>UpBlock 6</i>. Our empirical evaluations prove that our decoder based on upsampling and
  convolution leads to better performance in all the metrics compared to the
  deconvolution-based decoder employed in existing studies
  [<a href="#reference" style="color: green; text-decoration: none;">12</a>]
  and [<a href="#reference" style="color: green; text-decoration: none;">1</a>].
</p>
<p>
  <div id="table-5" style="width: 800px; height: 330px; margin: 0 auto;">
    <img src="./figures/table-5.png" width="800"/>
  </div>
</p>

<h3>4.2.2 Multi-resolution Decoder</h3>
<p>
  Inspired by [<a href="#reference" style="color: green; text-decoration: none;">14</a>],
  the multi-resolution decoder is optimized to generate images at both low resolution
  and high resolution, which is shown in Figure <a href="#figure-3" style="color: red; text-decoration: none;">3</a>.
  With the multi-resolution approach, the decoder learns to model the multiple
  target domain distributions of different scales, which helps to overcome
  the difficulty of generating high-resolution images.
</p>
<p>
  <div id="figure-3" style="width: 800px; height: 260px; margin: 0 auto;">
    <img src="./figures/figure-3.png" width="800"/>
  </div>
</p>

<h3>4.3 Discriminators</h3>
<p>
  The network structure of image discriminator <i>D<sub>real</sub></i> and identity classifier <i>D<sub>id</sub></i> is described in Table
  <a href="#table-6" style="color: red; text-decoration: none;">6</a>. Both networks are convolutional neural networks, followed by fully connected networks. In Table
  <a href="#table-6" style="color: red; text-decoration: none;">6</a>, we demonstrate the structure of discriminators for 64 × 64 images, and the discriminator of 128 × 128
  images can be simply implemented by adding another convolution layer before average pooling layer.
</p>
<p>
  <div id="table-6" style="width: 850px; height: 320px; margin: 0 auto;">
    <img src="./figures/table-6.png" width="850"/>
  </div>
</p>

<h3>4.4 Complexity Analysis of SF2F model</h3>
<p>
  Shown in Table <a href="#table-7" style="color: red; text-decoration: none;">7</a>
  is the time and space complexity of each component of SF2F model, where <i>t</i> stands
  for the length of the input audio and <i>w</i> stands for the width of the output face image.
  The time complexity of convolution and attention is &Omega;(1) regardless of the input length.
  The space complexity of 1D convolution is <i>O(t)</i> and space complexity of attention is <i>O(t<sup>2</sup>)</i>.
</p>
<p>
  <div id="table-7" style="width: 550px; height: 170px; margin: 0 auto;">
    <img src="./figures/table-7.png" width="550"/>
  </div>
</p>
<p><a href="#">To the top.</a></p>
<p>&nbsp;</p>


<!-- Implementation Details -->
<h2><a id="imple"></a>
  5 Implementation Details
</h2>
<h3>5.1 Training</h3>
<p>
  We train all our SF2F models on 1 NVIDIA V100 GPU with 32 GB memory; SF2F is implemented
  with PyTorch [<a href="#reference" style="color: green; text-decoration: none;">15</a>].
  The encoder-decoder training takes 120,000 iterations, and the fuser training
  takes 360 iterations. The training of 64 × 64 models takes about 18 hours,
  and the training of 128 × 128 models takes about 32 hours. The model is trained only once in each experiment.
</p>

<h3>5.2 Evaluation</h3>
<p>
  Evaluation is conducted on the same GPU machine. The details of the implementation
  are provided in this part of the section.
</p>
<p>
  <b>Ground Truth Embedding Matrix.</b> As mentioned in the main paper, the ground truth embedding
  matrix `U = {u_1, u_2, ..., u_N}` extracted by FaceNet [<a href="#reference" style="color: green; text-decoration: none;">16</a>] is used for similarity metric and retrieval
  metric. Given that one identity is often associated with multiple, i.e., <i>K</i>, face images in both datasets,
  for an identity of index <i>n</i>, the ground truth embedding is computed by `sum_(j=1)^K u_(nj)/K`. The
  embedding `u_n` is normalized as `u_n = frac{u_n}{max(||u_n||_2, &#949;)}` , because the embeddings extracted by FaceNet
  are also L2 normalized. Building embedding matrix with all the image data helps remove variance in
  data. This makes the evaluation results fair and stable.
</p>
<p>
  <b>Evaluation Runs.</b> In each experiment, we randomly crop 10 pieces of audio with desired length
  for each identity in the evaluation dataset. With the data above, each experiment is evaluated <i>ten</i>
  times, the mean of each metric is calculated based on the outcomes of all these ten evaluation runs.
  We additionally report the variance of VGGFace Score.
</p>

<h3>5.3 Hyperparameter</h3>
<p>
  The hyperparameters for SF2F’s model training are listed in Table <a href="#table-8" style="color: red; text-decoration: none;">8</a>. The detailed configuration of
  SF2F’s network is available in Section <a href="#model" style="color: red; text-decoration: none;">4</a> as well as the main paper.
</p>
<p>
  <div id="table-8" style="width: 300px; height: 300px; margin: 0 auto;">
    <img src="./figures/table-8.png" width="300"/>
  </div>
</p>
<p>
  Hyperparameters are carefully tuned. As the training is time-consuming and the hyperparameter
  space is large, it is difficult to apply grid search directly. We adopt other methods for parameter
  tuning instead.
</p>
<p>
  We first train a SF2F model with &lambda;<sub>1</sub>, &lambda;<sub>2</sub>, &lambda;<sub>3</sub> = 1 and &lambda;<sub>4</sub> = 0, we adjust the parameters &lambda;<sub>1</sub> and &lambda;<sub>3</sub> and
  find that increasing the Image Reconstruction Loss weight &lambda;<sub>1</sub> and decreasing the Auxiliary Classifier
  Loss weight &lambda;<sub>3</sub> both improve the model performance. We adjust &lambda;<sub>1</sub> and &lambda;<sub>3</sub> gradually and find out
  the model achieves the best performance when &lambda;<sub>1</sub> = 10 and &lambda;<sub>3</sub> = 0.05. Afterward, we apply
  Perceptual Loss to our model training with initial weight &lambda;<sub>4</sub> = 1. We find increasing &lambda;<sub>4</sub> improves
  SF2F’s performance, which is because the original scale of Perceptual Loss is much smaller than the
  scale of the other three losses. We gradually increase &lambda;<sub>4</sub> until we observe SF2F achieving the best
  performance when &lambda;<sub>4</sub> = 100. Therefore, &lambda;<sub>1</sub>, &lambda;<sub>2</sub>, &lambda;<sub>3</sub>, &lambda;<sub>4</sub> are set at 10, 1, 0.05 and 100, respectively.
</p>
<p>
  Afterwards, we apply a grid search over learning rate and batch size. We test with different values,
  which are listed in Table <a href="#table-9" style="color: red; text-decoration: none;">9</a> with optimal values singled out in the last column.
</p>
<p>
  The hyperparameters above are determined by tuning a 64 × 64 SF2F model with HQ-VoxCeleb
  dataset, and we find this hyperparameter configuration works well with 128 × 128 SF2F model.
  With this hyperparameter configuration, SF2F outperforms voice2face on filtered VGGFace dataset.
</p>
<p>
  <div id="table-9" style="width: 500px; height: 130px; margin: 0 auto;">
    <img src="./figures/table-9.png" width="500"/>
  </div>
</p>
<p>
  Consequently, we opt to skip further hyperparameter tuning on filtered VGGFace. The total cost of hyperparameter tuning is around 558 GPU hours on V100.
</p>
<p><a href="#">To the top.</a></p>
<p>&nbsp;</p>


<h2>References</h2>
<p id="reference">
  <ol>
     <li>
        Yandong Wen, Bhiksha Raj, and Rita Singh.:
        <cite>Face reconstruction from voice using generative adversarial networks.</cite>
        In <i>Advances in Neural Information Processing Systems</i>, pages 5266–5275, 2019.
     </li>
     <li>
        Yandong Wen, Bhiksha Raj, and Rita Singh.:
        <cite>Implementation of reconstructing faces from voices paper.</cite>
        <a>https://github.com/cmu-mlsp/reconstructing_faces_from_voices.</a> Accessed: 2020-5-29.
     </li>
     <li>
       Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman.:
       <cite>Deep face recognition.</cite>
       2015.
     </li>
     <li>
       Octavio Arriaga, Matias Valdenegro-Toro, and Paul Plöger.:
       <cite>Real-time convolutional neural networks for emotion and gender classification.</cite>
       <i>arXiv preprint arXiv:1710.07557</i>, 2017.
     </li>
     <li>
       Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.:
       <cite>Pyramid scene parsing network.</cite>
       In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i>, pages 2881–2890, 2017.
     </li>
     <li>
       M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.:
       <cite>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.</cite>
       <a>http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.</a>
     </li>
     <li>
       Shota Horiguchi, Naoyuki Kanda, and Kenji Nagamatsu.:
       <cite>Face-voice matching using cross-modal embeddings.</cite>
       In <i>Proceedings of the 26th ACM international conference on Multimedia</i>, pages 1011–1019, 2018.
     </li>
     <li>
       Changil Kim, Hijung Valentina Shin, Tae-Hyun Oh, Alexandre Kaspar, Mohamed Elgharib, and Wojciech Matusik.:
       <cite>On learning associations of faces and voices.</cite>
       In <i>Asian Conference on Computer Vision,</i>, pages 276–292. Springer, 2018.
     </li>
     <li>
       Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein.:
       <cite>Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation.</cite>
       <i>arXiv preprint arXiv:1804.03619</i>, 2018.
     </li>
     <li>
       Arsha Nagrani, Joon Son Chung, and Andrew Zisserman:
       <cite>Voxceleb: A large-scale speaker identification dataset.</cite>
       In <i>Interspeech</i>, pages 2616–2620, 2017.
     </li>
     <li>
       Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.:
       <cite>Voxceleb2: Deep speaker recognition.</cite>
       In <i>Interspeech</i>, pages 1086–1090, 2018.
     </li>
     <li>
       Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T. Freeman, Michael Rubinstein, and Wojciech Matusik.:
       <cite>Speech2face: Learning the face behind a voice.</cite>
       In <i>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, June 2019.
     </li>
     <li>
       Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.:
       <cite>Going deeper with convolutions.</cite>
       In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i>, pages 1–9, 2015.
     </li>
     <li>
       Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas.:
       <cite>Stackgan++: Realistic image synthesis with stacked generative adversarial networks.</cite>
       <i>IEEE transactions on pattern analysis and machine intelligence</i>, 41(8):1947–1962, 2018.
     </li>
     <li>
       Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.:
       <cite>Pytorch: An imperative style, high-performance deep learning library.</cite>
       In <i>Advances in Neural Information Processing Systems</i>, pages 8024–8035, 2019.
     </li>
     <li>
       Florian Schroff, Dmitry Kalenichenko, and James Philbin.:
       <cite>Facenet: A unified embedding for face recognition and clustering.</cite>
       In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i>, pages 815–823, 2015.
     </li>
     <li>
       Diederik P. Kingma and Jimmy Ba.:
       <cite>Adam: A method for stochastic optimization.</cite>
       In <i>ICLR</i>, 2015.
     </li>
  </ol>
</p>


<!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=--->
</html>
