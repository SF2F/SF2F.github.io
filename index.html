<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>NeurIPS'20| Speech Fusion to Face</title>
<link href="css/style.css" rel="stylesheet" type="text/css" />
</head>

<body>
<div class="container">
<h3 align="center" style="font-family:Times-New-Roman;">
  Submission to Neural Information Processing Systems (NeurIPS) 2020
</h3>

<h1 align="center" style="font-family:Times-New-Roman;">
  Speech Fusion to Face: Bridging the Gap Between Human’s Vocal Characteristics and Facial Imaging
</h1>

<h2 align="center" style="font-family:Times-New-Roman;">
  Supplementary Material
</h2>

<p align="center">&nbsp;</p>

<p style="font-family:Times-New-Roman;">
In the main paper, we present a state-of-the-art algorithm for automatic
generation of facial images based on the vocal characteristics extracted from
human speech.
In this supplementary, we show the input audio that cannot be included
in the main paper and additional qualitative results.
The input audio can be played in the browser (tested on Chrome and HTML5).
</p>

<p>&nbsp;</p>

<!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=--->
<h2><a id="part1"></a>
  Part 1: Qualitative Results of Speech-to-face Reconstrution with SF2F
</h2>
<table border="0" align="center">
  <tbody>
    <tr align="center">
      <td>&nbsp;</td>
      <td><strong>True face<br />
          (for reference)</strong></td>
      <td><strong>Face reconstructed<br />
        from speech</strong></td>
      <td width="24">&nbsp;</td>
      <td><strong>True face<br />
        (for reference)</strong></td>
      <td><strong>Face reconstructed<br />
        from speech</strong></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Aarti_Chhabria/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Aarti_Chhabria/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Adam_Senn/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Adam_Senn/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Aarti_Chhabria/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Adam_Senn/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Alaina_Huffman/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Alaina_Huffman/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Annet_Mahendru/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Annet_Mahendru/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Alaina_Huffman/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Annet_Mahendru/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Dean_Morgan/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Dean_Morgan/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Diego_Botto/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Diego_Botto/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Dean_Morgan/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Diego_Botto/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Apolo_Ohno/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Apolo_Ohno/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Ashley_Tisdale/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Ashley_Tisdale/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Apolo_Ohno/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Ashley_Tisdale/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Bianca_Guaccero/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Bianca_Guaccero/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/CariDee_English/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/CariDee_English/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Bianca_Guaccero/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/CariDee_English/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Dominic_West/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Dominic_West/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Emiliano_Viviano/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Emiliano_Viviano/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Dominic_West/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Emiliano_Viviano/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Eric_Bana/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Eric_Bana/gen.png" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/mid/Carolina_Crescentini/gt.jpg" width="150" /></td>
      <td><img src="examples/mid/Carolina_Crescentini/gen.png" width="150" /></td>
    </tr>
    <tr>
      <td><strong>Input<br />
      speech</strong></td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Eric_Bana/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
      <td>&nbsp;</td>
      <td colspan="2"><audio controls="controls" style="width: 300px;">
          <source src="examples/mid/Carolina_Crescentini/input.wav" type="audio/wav" />
          Does not support </audio></td>
      <!-- <td>&nbsp;</td> -->
    </tr>
  </tbody>
</table>
<p>
  <b>128 × 128 results on the VoxCeleb dataset.</b>
  Several results of SF2F are shown.
  SF2F can be genearlized from 64 × 64 pixel reconstruction to higher resolutions.
  Our method works for various lengths of input audio.
</p>
<p><a href="#">To the top.</a></p>
<p>&nbsp;</p>
<!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=--->

<h2><a id="part2"></a>Part 2: Additional Comparison between HQ-VoxCeleb and Filtered VGGFace Datasets</h2>
<table border="0" align="center">
  <tbody>
    <tr align="center">
      <td>&nbsp;</td>
      <td colspan="2">
        <strong><font size="4">HQ-VoxCeleb<font><br /></strong>
      </td>
      <td width="40">&nbsp;</td>
      <td colspan="2">
        <strong><font size="4">Filtered VGGFace<font><br /></strong>
      </td>
      <td width="40">&nbsp;</td>
      <td colspan="2">
        <strong><font size="4">Problems with Filtered VGGFace<font>
        <br /></strong>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Alex_Borstein/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Alex_Borstein/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Alex_Borstein/lq/3.jpg" width="125" /></td>
      <td><img src="examples/data/Alex_Borstein/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Expression</li>
          <li>Background</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Andrea_Riseborough/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Andrea_Riseborough/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Andrea_Riseborough/lq/1.jpg" width="125" /></td>
      <td><img src="examples/data/Andrea_Riseborough/lq/5.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Background</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Andrew_Rannells/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Andrew_Rannells/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Andrew_Rannells/lq/1.jpg" width="125" /></td>
      <td><img src="examples/data/Andrew_Rannells/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Expression</li>
          <li>Background</li>
          <li>Resolution</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Anthony_Anderson/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Anthony_Anderson/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Anthony_Anderson/lq/5.jpg" width="125" /></td>
      <td><img src="examples/data/Anthony_Anderson/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Expression</li>
          <li>Background</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Bear_Grylls/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Bear_Grylls/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Bear_Grylls/lq/1.jpg" width="125" /></td>
      <td><img src="examples/data/Bear_Grylls/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Expression</li>
          <li>Background</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Emraan_Hashmi/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Emraan_Hashmi/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Emraan_Hashmi/lq/3.jpg" width="125" /></td>
      <td><img src="examples/data/Emraan_Hashmi/lq/0.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Background</li>
          <li>Lighting condition</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Fawad_Khan/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Fawad_Khan/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Fawad_Khan/lq/4.jpg" width="125" /></td>
      <td><img src="examples/data/Fawad_Khan/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Background</li>
          <li>Resolution</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><img src="examples/data/Freida_Pinto/hq/1.jpg" width="150" /></td>
      <td><img src="examples/data/Freida_Pinto/hq/2.jpg" width="150" /></td>
      <td>&nbsp;</td>
      <td><img src="examples/data/Freida_Pinto/lq/1.jpg" width="125" /></td>
      <td><img src="examples/data/Freida_Pinto/lq/2.jpg" width="125" /></td>
      <td>&nbsp;</td>
      <td>
        <ul style="list-style-type:disc;">
          <li>Face angle</li>
          <li>Background</li>
          <li>Lighting condition</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
<p>
  The quality variance of the original manually filtered VGGFace dataset (on the
   right of the figure) [1] diverts the efforts of face
   generator model to the normalization of the faces. By filtering and
   correcting the face database (on the left), the computer vision models are
   expected to focus more on the construction of mapping between vocal features
   and physical facial features.
</p>
<p><a href="#">To the top.</a></p>
<p>&nbsp;</p>

<h2><a id="data-quality"></a>
  1 Qualitative Results
</h2>
<p>
  In the main paper, we compare the 64 × 64 face images generated by SF2F and
  voice2face [<a href="#" style="color: green; text-decoration: none;">1</a>].
  In this section, we compare the 128 × 128 face images reconstructed by both models.
  As the original voice2face [<a href="#" style="color: green; text-decoration: none;">1</a>, <a href="#" style="color: green; text-decoration: none;">2</a>]
  is only trained to generate 64 × 64 images, we compare SF2F and voice2face trained
  on HQ-VoxCeleb. For the sake of fairness, both models are trained until convergence, and checkpoints
  with the best L1 similarity is used for inference. As shown in Figure <a href="#figure-2" style="color: red; text-decoration: none;">1</a>,
  although voice2face can capture attributes such as gender, SF2F generates images
  with much more accurate facial features and face shape. The pose, expression,
  and lighting over the faces from SF2F are generally more stable and consistent than
  the face images from voice2face. In group (f), for example, SF2F predicts
  a face almost identical to the ground truth, when the corresponding output from
  voice2face is hardly recognizable. This proves our model design enables more accurate
  information flow from the speech domain to the face domain.
</p>
<p>
  <div id="figure-1" style="width: 800px; height: 570px; margin: 0 auto;">
    <img src="./figures/figure-1.png" width="800"/>
  </div>
</p>


<h2><a id="data-quality"></a>
  2 Data Quality Enhancement
</h2>
<p>
  An overview of the HQ-VoxCeleb dataset is presented in the main paper.
  In this section, we elaborate the standards and process of data enhancement.
  As demonstrated in Figure <a href="#figure-2" style="color: red; text-decoration: none;">2</a>,
  the poor quality of training dataset for speech2face is one of the major factors
  hindering the improvement of <i>speech2face</i> performance. To eliminate the
  negative impact of the training dataset, we carefully design and build a new
  high-quality face database on top of VoxCeleb dataset, such that face images
  associated with the celebrities are all at reasonable quality.
</p>
<p>
  <div id="figure-2" style="width: 900px; height: 750px; margin: 0 auto;">
    <img src="./figures/figure-2.png" width="900"/>
  </div>
</p>
<p>
  To fulfill the vision of data quality, we set a number of general guidelines
  over the face images as the underlying measurement on <i>quality</i>, as follows:
  <ul>
    <li><b>Face angle</b> between human’s facial plane and the photo imaging plane is no larger than 5&deg;;</li>
    <li><b>Lighting condition</b> on the face is generally uniform, without any obvious shadow and sharp
illumination change;</li>
    <li><b>Expression</b> on the human faces is generally neutral, while minor smile expression is also acceptable;</li>
    <li><b>Background</b> does not contain irrelevant information in the image, and completely void background in white is preferred.</li>
  </ul>
</p>
<p>
  To fully meet the standards as listed above, we adopt the following methodology to build the enhanced
  <i>HQ-VoxCeleb</i> dataset for our <i>speech2face</i> model training.
</p>
<p>
  <b>Data Collection.</b> We collect 300 in-the-wild images for each of the 7,363
  individuals in VoxCeleb dataset, by crawling images of the celebrities on the
  Internet. The visual qualities of the retrieved images are highly diverse.
  The resolution of the images, for example, ranges from 47 × 59 to 6245 × 8093 in pixels.
  Moreover, some of the images cover the full body of the celebrity of interest,
  while other images only include the face of the target individual.
  It is, therefore, necessary to apply pre-processing and filtering operations
  to ensure 1) the variance of the image quality is reasonably small; and 2) all
  the images are centered at the face of the target individuals.
</p>
<p>
  <b>Machine Filtering.</b> To filter out unqualified images from the massive in-the-wild
  images, we deploy an automated filtering module, together with a suite of concrete
  selection rules, to eliminate images at poor quality, before dispatching the images
  for human filtering. In the filtering module, the algorithm first detects the
  landmarks from the raw face images. Based on the output landmarks of the faces,
  the algorithm identifies poorly posed faces, if the landmarks from left/right
  sides cannot be well aligned. Low-resolution face images with distance between
  pupils covers fewer than 30 pixels are also removed. Finally, a pre-trained CNN
  classifier [<a href="#" style="color: green; text-decoration: none;">4</a>] is
  deployed to infer the emotion of the individual in the image, such that faces
  not recognized as "neutral” emotion are also removed from the dataset.
</p>
<p>
  <b>Image Processing.</b> Given the face images passing the first round machine filtering,
  we apply a two-step image processing, namely face alignment and image segmentation.
  In the first step of face alignment, the images are rotated and cropped to make
  sure both the pupils of the faces in all these images are always at the same coordinates.
  In the second step of image segmentation, we apply a pyramid scene parsing
  network [<a href="#" style="color: green; text-decoration: none;">5</a>] pre-trained
  on Pascal VOC 2012 dataset [<a href="#" style="color: green; text-decoration: none;">6</a>]
  to split the target individual from the background in the image. The background
  is then refilled with white pixels. Note that the second step is helpful because
  irrelevant noise in the background may potentially confuse the generation model.
</p>
<p>
  <b>Human Filtering.</b> To guarantee the final quality of the images in HQ-VoxCeleb dataset, human
  workers are employed to select 3 to 7 images at best qualities for each celebrity identity, who has at
  least 3 remaining face images after the machine filtering.
</p>
<p>
  In the main paper, we summarize the statistics of the result dataset after the
  adoption of the processing steps above.
</p>

<!-- Ablation Study -->
<h2><a id="ablation"></a>
  3 Ablation Study
</h2>
<p>
  In this section, we focus on the importance evaluation of components, loss functions,
  and dataset used in our SF2F model. The ablated models are trained to generate
  64 × 64 images, an all the results are summarized in
  Table <a href="#table-2" style="color: red; text-decoration: none;">1</a>.
  When removing any of the four loss functions in {<i>L<sub>1</sub></i>, <i>L<sub>G</sub></i>,
  <i>L<sub>C</sub></i> , <i>L<sub>P</sub></i>}, the performance of SF2F drops accordingly.
  This shows it is necessary to include all these components to ensure the learning
  procedure of the model is balanced. 1D-CNN encoder is used in the baseline
  approach [<a href="#" style="color: green; text-decoration: none;">1</a>],
  while SF2F employs 1D-Inception encoder instead. We report the performance of
  SF2F by replacing our encoder with 1D-CNN, referred to as Baseline Encoder in
  Table <a href="#table-2" style="color: red; text-decoration: none;">1</a>. This
  replacement causes a significant downgrade of the performance on all metrics. Similarly, by adopting
  the deconvolution-based decoder used in [<a href="#" style="color: green; text-decoration: none;">1</a>, <a href="#" style="color: green; text-decoration: none;">7</a>]
  instead of the upsampling-and-convolution-based decoder in SF2F, referred as
  Baseline Decoder in Table <a href="#table-2" style="color: red; text-decoration: none;">1</a>,
  we can observe a slight but consistent performance drop.
  The impact of data quality is also evaluated, by training the SF2F model with
  the manually filtered version of VGGFace dataset [<a href="#" style="color: green; text-decoration: none;">3</a>]
  instead of HQ-VoxCeleb, by including all the celebrity individuals included in both datasets.
  Poor quality leads to a huge performance plunge, which justifies the importance of training data quality.
</p>
<p>
  <div id="table-1" style="width: 600px; height: 300px; margin: 0 auto;">
    <img src="./figures/table-1.png" width="600"/>
  </div>
</p>



<!-- Model Details -->
<h2><a id="model"></a>
  4 Model Details
</h2>
<h3>4.1 Voice Encoder</h3>
<p>
  We use a 1D-CNN composed of several 1D Inception modules
  [<a href="#" style="color: green; text-decoration: none;">8</a>] to process mel-spectrogram.
  The voice encoder module converts each short input speech segment into a predicted
  facial feature embedding. The architecture of the 1D Inception module and the
  voice encoder is summarized in Table <a href="#table-2" style="color: red; text-decoration: none;">2</a>.
  The Inception module models various ranges
  of short-term mel-spectrogram dependency, and enables more accurate information
  flow from voice domain to image domain, compared to plain single-kernel-size CNN
  used in [<a href="#table-5" style="color: green; text-decoration: none;">1</a>].
</p>
<p>
  <div id="table-2" style="width: 800px; height: 280px; margin: 0 auto;">
    <img src="./figures/table-2.png" width="800"/>
  </div>
</p>

<h3>4.2 Face Decoder</h3>
<h3>4.2.1 Single-resolution Decoder</h3>
<p>
  The face decoder reconstructs the target individual’s face image based on the
  embeddings extracted from the individual’s speech segments. The architectures
  of the upsampling block (UpBlock) and the face decoder are summarized in
  Table <a href="#table-3" style="color: red; text-decoration: none;">3</a>.
  We show the structure of the face decoder that generates 64 × 64 images, and the
  face decoder generates 128 × 128 images can be built by adding an UpBlock 7 after
  UpBlock 6. Our empirical evaluations prove that our decoder based on upsampling and
  convolution leads to better performance in all the metrics compared to the
  deconvolution-based decoder employed in existing studies
  [<a href="#" style="color: green; text-decoration: none;">7</a>]
  and [<a href="#" style="color: green; text-decoration: none;">1</a>].
</p>
<p>
  <div id="table-3" style="width: 800px; height: 320px; margin: 0 auto;">
    <img src="./figures/table-3.png" width="800"/>
  </div>
</p>

<h3>4.2.2 Multi-resolution Decoder</h3>
<p>
  Inspired by [<a href="#" style="color: green; text-decoration: none;">9</a>],
  the multi-resolution decoder is optimized to generate images at both low resolution
  and high resolution, which is shown in Figure <a href="#figure-3" style="color: red; text-decoration: none;">3</a>.
  With the multi-resolution approach, the decoder learns to model the multiple
  target domain distributions of different scales, which helps to overcome
  the difficulty of generating high-resolution images.
</p>
<p>
  <div id="figure-3" style="width: 800px; height: 260px; margin: 0 auto;">
    <img src="./figures/figure-3.png" width="800"/>
  </div>
</p>

<h3>4.3 Complexity Analysis</h3>
<p>
  Shown in Table <a href="#table-4" style="color: red; text-decoration: none;">4</a>
  is the time and space complexity of each component of SF2F model, where <i>t</i> stands
  for the length of the input audio and <i>w</i> stands for the width of the output face image.
  Convolution and attention have time complexity &Omega;(1) regardless of input length.
  1D convolution has space complexity <i>O(t)</i> and self attention has space complexity <i>O(t<sup>2</sup>)</i>.
</p>
<p>
  <div id="table-4" style="width: 600px; height: 170px; margin: 0 auto;">
    <img src="./figures/table-4.png" width="600"/>
  </div>
</p>


<!-- Implementation Details -->
<h2><a id="imple"></a>
  5 Implementation Details
</h2>
<p>
  The hyperparameters used during SF2F’s model training is summarized in Table
  <a href="#table-5" style="color: red; text-decoration: none;">5</a>.
  The configuration of SF2F's network is detailedly introducted in Section 4 and
  the main paper.
</p>
<p>
  <div id="table-5" style="width: 300px; height: 300px; margin: 0 auto;">
    <img src="./figures/table-5.png" width="300"/>
  </div>
</p>



<!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=--->
</body>

<head>
   <title>References</title>
   </head>
   <body>
      <h1>References</h1>
      <ol>
         <li>
            Yandong Wen, Bhiksha Raj, and Rita Singh.:
            <cite>Face reconstruction from voice using generative adversarial networks.</cite>
            In <i>Advances in Neural Information Processing Systems</i>, pages 5266–5275, 2019.
         </li>
         <li>
            Yandong Wen, Bhiksha Raj, and Rita Singh.:
            <cite>Implementation of reconstructing faces from voices paper.</cite>
            <a>https://github.com/cmu-mlsp/reconstructing_faces_from_voices.</a> Accessed: 2020-5-29.
         </li>
         <li>
           Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman.:
           <cite>Deep face recognition.</cite>
           2015.
         </li>
         <li>
           Octavio Arriaga, Matias Valdenegro-Toro, and Paul Plöger.:
           <cite>Real-time convolutional neural networks for emotion and gender classification.</cite>
           <i>arXiv preprint arXiv:1710.07557</i>, 2017.
         </li>
         <li>
           Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.:
           <cite>Pyramid scene parsing network.</cite>
           In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i>, pages 2881–2890, 2017.
         </li>
         <li>
           M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.:
           <cite>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.</cite>
           <a>http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.</a>
         </li>
         <li>
           Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T. Freeman, Michael Rubinstein, and Wojciech Matusik.:
           <cite>Speech2face: Learning the face behind a voice.</cite>
           In <i>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, June 2019.
         </li>
         <li>
           Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.:
           <cite>Going deeper with convolutions.</cite>
           In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i>, pages 1–9, 2015.
         </li>
         <li>
           Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas.:
           <cite>Stackgan++: Realistic image synthesis with stacked generative adversarial networks.</cite>
           <i>IEEE transactions on pattern analysis and machine intelligence</i>, 41(8):1947–1962, 2018.
         </li>
         <li>
           Diederik P. Kingma and Jimmy Ba.:
           <cite>Adam: A method for stochastic optimization.</cite>
           In <i>ICLR</i>, 2015.
         </li>
      </ol>
   </body>
</html>
